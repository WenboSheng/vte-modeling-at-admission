{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import yaml\n",
    "import joblib\n",
    "import dill \n",
    "import numpy as np\n",
    "import scipy as sp \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.utils import shuffle\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utility import load_pickle, read_table_file, save_pickle, save_dataframe_dict\n",
    "from evaluation.calibration import calibration_test\n",
    "from training.model_training import run_automl_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['pdf.use14corefonts'] = True\n",
    "plt.rc('font', family='Helvetica')\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "# plt.rcParams['font.sans-serif'] = ['Helvetica']\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xydata_dir = 'data/cohorts_for_learning/xydata_220913'\n",
    "dataset_4d_caprini_dict = load_pickle(os.path.join(xydata_dir, f'dataset_4d_scheme3_caprini.pkl'))\n",
    "\n",
    "task_4d_caprini_df = pd.concat(\n",
    "    [\n",
    "        dataset_4d_caprini_dict['x_data'],\n",
    "        dataset_4d_caprini_dict['y_data'],\n",
    "        dataset_4d_caprini_dict['misc_data'][['dataset', 'outcome']]\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "task_4d_caprini_df.loc[task_4d_caprini_df['dataset'] == 'test', 'dataset'] = 'inner_test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = task_4d_caprini_df[task_4d_caprini_df.dataset == 'train'].index\n",
    "test_idx = task_4d_caprini_df[task_4d_caprini_df.dataset == 'inner_test'].index\n",
    "caprini_feats = task_4d_caprini_df.columns[:-3]\n",
    "\n",
    "x_train, y_train = task_4d_caprini_df.loc[train_idx, caprini_feats], task_4d_caprini_df.loc[train_idx, 'label']\n",
    "x_test, y_test = task_4d_caprini_df.loc[test_idx, caprini_feats], task_4d_caprini_df.loc[test_idx, 'label']\n",
    "y_train.value_counts(), y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_config_file = 'data/automl/config.yml'\n",
    "_out_dir = 'test/caprini_automl/1011_1'\n",
    "save_dir = f\"{_out_dir}_additional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_automl_models(\n",
    "    task_4d_caprini_df.reset_index(), _config_file, _out_dir,\n",
    "    label_col='label', dataset_col='dataset', train_name='train', misc_cols=['visit_id', 'outcome']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection by Wrapping / Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models from automl tuning\n",
    "lasso_model_components_dict = joblib.load(\n",
    "    os.path.join(_out_dir, 'logistic_regression/predict_model_components.pkl')\n",
    ")\n",
    "lasso_model = lasso_model_components_dict['model'].model\n",
    "\n",
    "xgb_model_components_dict = joblib.load(\n",
    "    os.path.join(_out_dir, 'xgboost/predict_model_components.pkl')\n",
    ")\n",
    "xgb_model = xgb_model_components_dict['model'].model\n",
    "\n",
    "rf_model_components_dict = joblib.load(\n",
    "    os.path.join(_out_dir, 'random_forest/predict_model_components.pkl')\n",
    ")\n",
    "rf_model = rf_model_components_dict['model'].model\n",
    "\n",
    "print(list(lasso_model_components_dict))\n",
    "print(list(xgb_model_components_dict))\n",
    "print(xgb_model.get_params())\n",
    "print(rf_model.get_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR by Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feats_lasso = np.array(lasso_model_components_dict['trained_features'])[lasso_model.coef_.flatten() != 0]\n",
    "lr0 = LogisticRegression(C=100, max_iter=1000).fit(x_train[feats_lasso], y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB with RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfe_xgb1 = RFECV(\n",
    "    xgb_model,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=2),\n",
    "    scoring='roc_auc', min_features_to_select=1\n",
    ")\n",
    "rfe_xgb1.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB进一步特征选择，提升泛化性，根据RFE做特征选择\n",
    "_feats_imp = pd.Series(\n",
    "    rfe_xgb1.estimator_.feature_importances_, index=rfe_xgb1.get_feature_names_out()\n",
    ") > 0\n",
    "feats_xgb1 = _feats_imp[_feats_imp].index.values\n",
    "\n",
    "xgb1 = XGBClassifier(**xgb_model.get_params()).fit(x_train[feats_xgb1], y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF with RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfe_rf1 = RFECV(\n",
    "    rf_model,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=2),\n",
    "    scoring='roc_auc', min_features_to_select=1\n",
    ")\n",
    "rfe_rf1.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_feats_mask = pd.Series(\n",
    "    rfe_rf1.estimator_.feature_importances_, index=rfe_rf1.get_feature_names_out()\n",
    ") > 0\n",
    "feats_rf1 = _feats_mask[_feats_mask].index.values\n",
    "\n",
    "rf1 = RandomForestClassifier(**rf_model.get_params()).fit(x_train[feats_rf1], y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caprini Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_caprini_score(x):\n",
    "    return 11 if x >= 11 else x\n",
    "\n",
    "\n",
    "caprini_score_train = dataset_4d_caprini_dict['score_data'].loc[train_idx, 'score'].rename('Caprini')\n",
    "caprini_score_test = dataset_4d_caprini_dict['score_data'].loc[test_idx, 'score'].rename('Caprini')\n",
    "caprini_mapped_score_train = caprini_score_train.apply(map_caprini_score)\n",
    "caprini_mapped_score_test = caprini_score_test.apply(map_caprini_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caprini_lr = LogisticRegression(penalty='none').fit(caprini_mapped_score_train.values.reshape([-1, 1]), y_train.values)\n",
    "caprini_scaled_score_train = pd.Series(\n",
    "    caprini_lr.predict_proba(caprini_mapped_score_train.values.reshape([-1, 1]))[:, 1],\n",
    "    index=caprini_mapped_score_train.index, name=caprini_mapped_score_train.name\n",
    ")\n",
    "caprini_scaled_score_test = pd.Series(\n",
    "    caprini_lr.predict_proba(caprini_mapped_score_test.values.reshape([-1, 1]))[:, 1],\n",
    "    index=caprini_mapped_score_test.index, name=caprini_mapped_score_test.name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_explainer1 = shap.TreeExplainer(rf1)\n",
    "rf_shap_values = rf_explainer1.shap_values(x_test[feats_rf1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP for XGB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_explainer1 = shap.TreeExplainer(xgb1)\n",
    "xgb_shap_values1 = xgb_explainer1.shap_values(x_test[feats_xgb1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration\n",
    "\n",
    "Calibrator for XGB, RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup: isotonic, ensemble=True, cv without shuffle\n",
    "\"\"\"\n",
    "\n",
    "xgb1_calibrator = CalibratedClassifierCV(\n",
    "    xgb1, method='isotonic', ensemble=True, cv=5  # StratifiedKFold(n_splits=5, shuffle=False)\n",
    ").fit(*shuffle(x_train[feats_xgb1], y_train, random_state=0))\n",
    "\n",
    "xgb_calib_pred_inner_test1 = xgb1_calibrator.predict_proba(x_test[feats_xgb1])[:, 1].flatten()\n",
    "xgb_calib_pred_train1 = xgb1_calibrator.predict_proba(x_train[feats_xgb1])[:, 1].flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf1 \n",
    "rf1_calibrator = CalibratedClassifierCV(\n",
    "    rf1, method='isotonic', ensemble=True, cv=10  # StratifiedKFold(n_splits=5, shuffle=False)\n",
    ").fit(*shuffle(x_train[feats_rf1], y_train, random_state=0))\n",
    "\n",
    "rf_calib_pred_train1 = rf1_calibrator.predict_proba(x_train[feats_rf1])[:, 1].flatten()\n",
    "rf_calib_pred_inner_test1 = rf1_calibrator.predict_proba(x_test[feats_rf1])[:, 1].flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_dataset_idx = y_train.index\n",
    "\n",
    "_lr_pred = pd.Series(\n",
    "    lr0.predict_proba(x_train[feats_lasso])[:, 1].flatten(), \n",
    "    index=_dataset_idx, name='LR'\n",
    ")\n",
    "\n",
    "_xgb_calib_pred = pd.Series(\n",
    "    xgb1_calibrator.predict_proba(x_train[feats_xgb1])[:, 1].flatten(), \n",
    "    index=_dataset_idx, name='XGB_Calib'\n",
    ")\n",
    "\n",
    "_rf_calib_pred = pd.Series(\n",
    "    rf1_calibrator.predict_proba(x_train[feats_rf1])[:, 1].flatten(),\n",
    "    index=_dataset_idx, name='RF_Calib'\n",
    ")\n",
    "\n",
    "predict_results_train = pd.concat(\n",
    "    [\n",
    "        y_train, caprini_score_train, caprini_scaled_score_train.rename('Caprini_Calib'), \n",
    "        _lr_pred, _xgb_calib_pred, _rf_calib_pred\n",
    "    ],\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_dataset_idx = y_test.index\n",
    "\n",
    "_lr_pred = pd.Series(\n",
    "    lr0.predict_proba(x_test[feats_lasso])[:, 1].flatten(), \n",
    "    index=_dataset_idx, name='LR'\n",
    ")\n",
    "\n",
    "_xgb_calib_pred = pd.Series(\n",
    "    xgb1_calibrator.predict_proba(x_test[feats_xgb1])[:, 1].flatten(), \n",
    "    index=_dataset_idx, name='XGB_Calib'\n",
    ")\n",
    "\n",
    "_rf_calib_pred = pd.Series(\n",
    "    rf1_calibrator.predict_proba(x_test[feats_rf1])[:, 1].flatten(),\n",
    "    index=_dataset_idx, name='RF_Calib'\n",
    ")\n",
    "\n",
    "predict_results_test = pd.concat(\n",
    "    [\n",
    "        y_test, caprini_score_test, caprini_scaled_score_test.rename('Caprini_Calib'), \n",
    "        _lr_pred, _xgb_calib_pred, _rf_calib_pred\n",
    "    ],\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = list(predict_results_test.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_roc_details_dict: model -> data.frame of fpr, tpr, threshold\n",
    "\n",
    "test_auc_dict = {\n",
    "    _model: metrics.roc_auc_score(\n",
    "        predict_results_test.label, predict_results_test[_model]\n",
    "    )\n",
    "    for _model in model_list\n",
    "}\n",
    "\n",
    "test_roc_details_dict = dict()\n",
    "for _model in model_list:\n",
    "    fpr, tpr, thre = metrics.roc_curve(\n",
    "        predict_results_test.label, predict_results_test[_model]\n",
    "    )\n",
    "    test_roc_details_dict[_model] = pd.DataFrame(\n",
    "        {'fpr': fpr, 'tpr': tpr, 'threshold': thre}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_roc_details_dict: model -> data.frame of fpr, tpr, threshold\n",
    "\n",
    "train_auc_dict = {\n",
    "    _model: metrics.roc_auc_score(\n",
    "        predict_results_train.label, predict_results_train[_model]\n",
    "    )\n",
    "    for _model in model_list\n",
    "}\n",
    "\n",
    "train_roc_details_dict = dict()\n",
    "for _model in model_list:\n",
    "    _fpr, _tpr, _thre = metrics.roc_curve(\n",
    "        predict_results_train.label, predict_results_train[_model]\n",
    "    )\n",
    "    train_roc_details_dict[_model] = pd.DataFrame(\n",
    "        {'fpr': _fpr, 'tpr': _tpr, 'threshold': _thre}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classif_metric_details: model -> data.frame of some common metrics w.r.t threshold\n",
    "\n",
    "classif_metric_details = dict()\n",
    "classif_metric_headings = [\n",
    "    'threshold',\n",
    "    'TN', 'FP', 'FN', 'TP',\n",
    "    'Sensitivity', 'Specificity', 'Precision', 'NPV', 'F1', 'F1(neg)', 'MCC', 'Youden', 'Gmean'\n",
    "]\n",
    "\n",
    "for _model in model_list:\n",
    "    _metric_details_list = []\n",
    "    for _thre in test_roc_details_dict[_model]['threshold'].values[-2:0:-1]:\n",
    "        _y_true = predict_results_test.label\n",
    "        _y_pred = (predict_results_test[_model] >= _thre).astype(float)\n",
    "        _tn, _fp, _fn, _tp = metrics.confusion_matrix(_y_true, _y_pred).ravel()\n",
    "        _p, _r, _f, _s = metrics.precision_recall_fscore_support(_y_true, _y_pred)\n",
    "        _mcc = metrics.matthews_corrcoef(_y_true, _y_pred)\n",
    "\n",
    "        _metric_details_list.append(\n",
    "            [\n",
    "                _thre,\n",
    "                _tn, _fp, _fn, _tp,\n",
    "                _r[1], _r[0], _p[1], _p[0],\n",
    "                _f[1], _f[0], _mcc,\n",
    "                _r[1] + _r[0] - 1,\n",
    "                np.sqrt(_r[1] * _r[0])\n",
    "            ]\n",
    "        )\n",
    "    classif_metric_details[_model] = pd.DataFrame(\n",
    "        _metric_details_list,\n",
    "        columns=classif_metric_headings\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classif_metric_details_train: model -> data.frame of some common metrics w.r.t threshold\n",
    "\n",
    "classif_metric_details_train = dict()\n",
    "\n",
    "for _model in model_list:\n",
    "    _metric_details_list = []\n",
    "    for _thre in train_roc_details_dict[_model]['threshold'].values[-2:0:-1]:\n",
    "        _y_true = predict_results_train.label\n",
    "        _y_pred = (predict_results_train[_model] >= _thre).astype(float)\n",
    "        _tn, _fp, _fn, _tp = metrics.confusion_matrix(_y_true, _y_pred).ravel()\n",
    "        _p, _r, _f, _s = metrics.precision_recall_fscore_support(_y_true, _y_pred, zero_division=0)\n",
    "        _mcc = metrics.matthews_corrcoef(_y_true, _y_pred)\n",
    "\n",
    "        _metric_details_list.append(\n",
    "            [\n",
    "                _thre,\n",
    "                _tn, _fp, _fn, _tp,\n",
    "                _r[1], _r[0], _p[1], _p[0],\n",
    "                _f[1], _f[0], _mcc,\n",
    "                _r[1] + _r[0] - 1,\n",
    "                np.sqrt(_r[1] * _r[0])\n",
    "            ]\n",
    "        )\n",
    "    classif_metric_details_train[_model] = pd.DataFrame(\n",
    "        _metric_details_list,\n",
    "        columns=classif_metric_headings\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk stratification\n",
    "### Caprini score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Caprini recall: test\n",
    "\"\"\"\n",
    "classif_metric_caprini_df = classif_metric_details['Caprini']\n",
    "\n",
    "thres_strat = [\n",
    "    (\n",
    "        'Sensitivity',\n",
    "        classif_metric_caprini_df.loc[classif_metric_caprini_df['threshold'] == 3, 'Sensitivity'].iloc[0]\n",
    "    ),\n",
    "    (\n",
    "        'Specificity',\n",
    "        classif_metric_caprini_df.loc[classif_metric_caprini_df['threshold'] == 5, 'Specificity'].iloc[0])\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Caprini recall: train\n",
    "\"\"\"\n",
    "classif_metric_caprini_df_train = classif_metric_details_train['Caprini']\n",
    "\n",
    "thres_strat_train = [\n",
    "    (\n",
    "        'Sensitivity',\n",
    "        classif_metric_caprini_df_train.loc[classif_metric_caprini_df_train['threshold'] == 3, 'Sensitivity'].iloc[0]\n",
    "    )\n",
    ",\n",
    "    (\n",
    "        'Specificity',\n",
    "        classif_metric_caprini_df_train.loc[classif_metric_caprini_df_train['threshold'] == 5, 'Specificity'].iloc[0])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dev_idx = task_4d_caprini_df.loc[task_4d_caprini_df.dataset.isin(('train', 'inner_test'))].index\n",
    "_train_idx = task_4d_caprini_df.loc[task_4d_caprini_df.dataset.isin(('train',))].index\n",
    "_test_idx = task_4d_caprini_df.loc[task_4d_caprini_df.dataset.isin(('inner_test',))].index\n",
    "\n",
    "caprini_level = dataset_4d_caprini_dict['score_data'].loc[_test_idx, 'score_level']\n",
    "caprini_level_train = dataset_4d_caprini_dict['score_data'].loc[_train_idx, 'score_level']\n",
    "\n",
    "y_test = dataset_4d_caprini_dict['y_data'].loc[_test_idx]\n",
    "y_train = dataset_4d_caprini_dict['y_data'].loc[_train_idx]\n",
    "\n",
    "prec_strat_caprini = pd.concat(\n",
    "   [caprini_level, y_test],\n",
    "   axis=1\n",
    ").groupby('score_level').apply(\n",
    "    lambda x: pd.Series(\n",
    "        {\n",
    "            'pos_rate': x['label'].sum() / len(x),\n",
    "            'level_count': len(x),\n",
    "            'level_proportion': len(x) / len(_test_idx),\n",
    "            'pos_count': x['label'].sum(),\n",
    "            'neg_count': len(x) - x['label'].sum()\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prec_strat_caprini_train = pd.concat(\n",
    "   [caprini_level_train, y_train],\n",
    "   axis=1\n",
    ").groupby('score_level').apply(\n",
    "    lambda x: pd.Series(\n",
    "        {\n",
    "            'pos_rate': x['label'].sum() / len(x),\n",
    "            'level_count': len(x),\n",
    "            'level_proportion': len(x) / len(_train_idx),\n",
    "            'pos_count': x['label'].sum(),\n",
    "            'neg_count': len(x) - x['label'].sum()\n",
    "        }\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_optimal_threshold(classif_metric_df, metric_name, metric_value, tol=1e-3, metric_backup='Youden'):\n",
    "    df1 = classif_metric_df.loc[classif_metric_df[metric_name] >= metric_value - tol]\n",
    "    df2 = df1.loc[df1[metric_name] == np.min(df1[metric_name])]\n",
    "    if len(df2) > 1:\n",
    "        idx_optimal = df2[metric_backup].idxmax()\n",
    "    else:\n",
    "        idx_optimal = df2.index[0]\n",
    "    return classif_metric_df.loc[idx_optimal, 'threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb calibration threshold: train\n",
    "tol_value = 1e-3\n",
    "\n",
    "classif_metric_xgb_calib_train_df = classif_metric_details_train['XGB_Calib']\n",
    "\n",
    "thres_strat_xgb_calib_train = [\n",
    "    search_optimal_threshold(classif_metric_xgb_calib_train_df, _metric, _val, tol_value)\n",
    "    for _metric, _val in thres_strat_train\n",
    "]\n",
    "\n",
    "xgb_calib_level_train = pd.cut(\n",
    "    predict_results_train['XGB_Calib'],\n",
    "    bins=[-1, thres_strat_xgb_calib_train[0], thres_strat_xgb_calib_train[1], 2],\n",
    "    right=False,\n",
    "    labels=['Low', 'Median', 'High']\n",
    ")\n",
    "\n",
    "prec_strat_xgb_calib_train = pd.DataFrame(\n",
    "    {'label': predict_results_train.label, 'XGB_Calib_level': xgb_calib_level_train}\n",
    ").groupby('XGB_Calib_level').apply(\n",
    "    lambda x: pd.Series(\n",
    "        {\n",
    "            'pos_rate': x['label'].sum() / len(x),\n",
    "            'level_count': len(x),\n",
    "            'level_proportion': len(x) / len(predict_results_train),\n",
    "            'pos_count': x['label'].sum(),\n",
    "            'neg_count': len(x) - x['label'].sum()\n",
    "        }\n",
    "    )\n",
    ")\n",
    "print(thres_strat_xgb_calib_train)\n",
    "prec_strat_xgb_calib_train\n",
    "\n",
    "# [0.07091680131852626, 0.10059409588575363]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb calibration threshold: test using train threshold\n",
    "\n",
    "xgb_calib_level2 = pd.cut(\n",
    "    predict_results_test['XGB_Calib'],\n",
    "    bins=[-1, thres_strat_xgb_calib_train[0], thres_strat_xgb_calib_train[1], 2],\n",
    "    right=False,\n",
    "    labels=['Low', 'Median', 'High']\n",
    ")\n",
    "\n",
    "prec_strat_xgb_calib2 = pd.DataFrame(\n",
    "    {'label': predict_results_test.label, 'XGB_Calib_level': xgb_calib_level2}\n",
    ").groupby('XGB_Calib_level').apply(\n",
    "    lambda x: pd.Series(\n",
    "        {\n",
    "            'pos_rate': x['label'].sum() / len(x),\n",
    "            'level_count': len(x),\n",
    "            'level_proportion': len(x) / len(predict_results_test),\n",
    "            'pos_count': x['label'].sum(),\n",
    "            'neg_count': len(x) - x['label'].sum()\n",
    "        }\n",
    "    )\n",
    ")\n",
    "# print(thres_strat_xgb_calib)\n",
    "prec_strat_xgb_calib2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf calibration threshold: train\n",
    "\n",
    "classif_metric_rf_calib_train_df = classif_metric_details_train['RF_Calib']\n",
    "thres_strat_rf_calib_train = [\n",
    "    search_optimal_threshold(classif_metric_rf_calib_train_df, _metric, _val, tol_value)\n",
    "    for _metric, _val in thres_strat_train\n",
    "]\n",
    "\n",
    "rf_calib_level_train = pd.cut(\n",
    "    predict_results_train['RF_Calib'],\n",
    "    bins=[-1, thres_strat_rf_calib_train[0], thres_strat_rf_calib_train[1], 2],\n",
    "    right=False,\n",
    "    labels=['Low', 'Median', 'High']\n",
    ")\n",
    "\n",
    "prec_strat_rf_calib_train = pd.DataFrame(\n",
    "    {'label': predict_results_train.label, 'RF_Calib_level': rf_calib_level_train}\n",
    ").groupby('RF_Calib_level').apply(\n",
    "    lambda x: pd.Series(\n",
    "        {\n",
    "            'pos_rate': x['label'].sum() / len(x),\n",
    "            'level_count': len(x),\n",
    "            'level_proportion': len(x) / len(predict_results_train),\n",
    "            'pos_count': x['label'].sum(),\n",
    "            'neg_count': len(x) - x['label'].sum()\n",
    "        }\n",
    "    )\n",
    ")\n",
    "print(thres_strat_rf_calib_train)\n",
    "prec_strat_rf_calib_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf calibration threshold: test using train threshold\n",
    "\n",
    "rf_calib_level2 = pd.cut(\n",
    "    predict_results_test['RF_Calib'],\n",
    "    bins=[-1, thres_strat_rf_calib_train[0], thres_strat_rf_calib_train[1], 2],\n",
    "    right=False,\n",
    "    labels=['Low', 'Median', 'High']\n",
    ")\n",
    "\n",
    "prec_strat_rf_calib2 = pd.DataFrame(\n",
    "    {'label': predict_results_test.label, 'RF_Calib_level': rf_calib_level2}\n",
    ").groupby('RF_Calib_level').apply(\n",
    "    lambda x: pd.Series(\n",
    "        {\n",
    "            'pos_rate': x['label'].sum() / len(x),\n",
    "            'level_count': len(x),\n",
    "            'level_proportion': len(x) / len(predict_results_test),\n",
    "            'pos_count': x['label'].sum(),\n",
    "            'neg_count': len(x) - x['label'].sum()\n",
    "        }\n",
    "    )\n",
    ")\n",
    "prec_strat_rf_calib2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test using train cutoff\n",
    "risk_strat_test = pd.concat([prec_strat_caprini, prec_strat_rf_calib2, prec_strat_xgb_calib2])\n",
    "risk_strat_test['model'] = ['Caprini'] * 3 + ['RandomForest'] * 3 + ['XGBoost'] * 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_strat_train = pd.concat([prec_strat_caprini_train, prec_strat_rf_calib_train, prec_strat_xgb_calib_train])\n",
    "risk_strat_train['model'] = ['Caprini'] * 3 + ['RandomForest'] * 3 + ['XGBoost'] * 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(\n",
    "    task_4d_caprini_df.loc[task_4d_caprini_df.dataset.isin(['train', 'inner_test'])], \n",
    "    os.path.join(save_dir, 'task_4d_caprini_df.pkl')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lr\n",
    "save_pickle(\n",
    "    {\n",
    "        'features_lasso': feats_lasso,\n",
    "        'lasso': lasso_model,\n",
    "        'lr_by_lasso': lr0\n",
    "    },\n",
    "    file=os.path.join(save_dir, 'lr', 'lr_components.pkl')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save xgboost\n",
    "\n",
    "save_pickle(\n",
    "    {\n",
    "        'features_xgb1': feats_xgb1,\n",
    "        'raw_xgb': xgb_model,\n",
    "        'xgb1': xgb1,\n",
    "        'xgb1_calibrator': xgb1_calibrator\n",
    "    },\n",
    "    file=os.path.join(save_dir, 'xgboost', 'xgb_components.pkl')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save rf\n",
    "\n",
    "save_pickle(\n",
    "    {\n",
    "        'features_rf1': feats_rf1,\n",
    "        'raw_rf': rf_model,\n",
    "        'rf1': rf1,\n",
    "        'rf1_calibrator': rf1_calibrator,\n",
    "    },\n",
    "    file=os.path.join(save_dir, 'rf', 'rf_components.pkl')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prediction values\n",
    "\n",
    "save_pickle(\n",
    "    {\n",
    "        'train': predict_results_train,\n",
    "        'test': predict_results_test\n",
    "    },\n",
    "    file=os.path.join(save_dir, 'prediction.pkl')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save shap explainer\n",
    "save_pickle(xgb_explainer1, file=os.path.join(save_dir, 'xgboost', 'shap_explainer.pkl'))\n",
    "save_pickle(rf_explainer1, file=os.path.join(save_dir, 'rf', 'shap_explainer.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save stratification\n",
    "save_dataframe_dict(\n",
    "    {'train': risk_strat_train, 'test': risk_strat_test},\n",
    "    file=os.path.join(save_dir, 'risk_stratification.xlsx')\n",
    ")\n",
    "save_pickle(\n",
    "    {'train': risk_strat_train, 'test': risk_strat_test},\n",
    "    file=os.path.join(save_dir, 'risk_stratification.pkl')\n",
    ")\n",
    "\n",
    "strat_level_details = {\n",
    "    'train': pd.DataFrame(\n",
    "        {\n",
    "            'Caprini': caprini_level_train,\n",
    "            'XGB_Calib': xgb_calib_level_train,\n",
    "            'RF_Calib': rf_calib_level_train\n",
    "        }\n",
    "    ),\n",
    "    'test': pd.DataFrame(\n",
    "        {\n",
    "            'Caprini': caprini_level,\n",
    "            'XGB_Calib': xgb_calib_level2,\n",
    "            'RF_Calib': rf_calib_level2\n",
    "        }\n",
    "    ),\n",
    "    'cutoff': pd.DataFrame(\n",
    "        {\n",
    "            'Caprini': [3, 5],\n",
    "            'XGB_Calib': thres_strat_xgb_calib_train,\n",
    "            'RF_Calib': thres_strat_rf_calib_train\n",
    "        }\n",
    "    )\n",
    "}\n",
    "\n",
    "save_dataframe_dict(\n",
    "    strat_level_details,\n",
    "    file=os.path.join(save_dir, 'risk_stratification_details.xlsx')\n",
    ")\n",
    "save_pickle(\n",
    "    strat_level_details,\n",
    "    file=os.path.join(save_dir, 'risk_stratification_details.pkl')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vte2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
